{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14f98f97",
   "metadata": {},
   "source": [
    "# Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e0dda620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast, pipeline, AutoTokenizer, AutoModel\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nlp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c595200",
   "metadata": {},
   "source": [
    "- Small-E-Czech is a transformer trained on Czech corpora by Seznam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89e5e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Seznam/small-e-czech\"\n",
    "# pipe = pipeline(task=\"text-classification\", model=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7b5600",
   "metadata": {},
   "source": [
    "# Data loading, manipulation and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0c15dc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V zoologické zahradě měli lachtana.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Když budu celý život tvrdě pracovat, tak si st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zrzavá liška běžela v lese.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dneska jsem viděl jak soused kácel strom.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matematika je lepší než zeměpis.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Slepice snáší vejce.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Včera večer jsme na procházce krmili labutě.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Nasadil jsem si brýle.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Večeřeli jsme v kuchyni.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kohout ráno všechny vzbudil.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                V zoologické zahradě měli lachtana.      1\n",
       "1  Když budu celý život tvrdě pracovat, tak si st...      0\n",
       "2                        Zrzavá liška běžela v lese.      1\n",
       "3          Dneska jsem viděl jak soused kácel strom.      0\n",
       "4                   Matematika je lepší než zeměpis.      0\n",
       "5                               Slepice snáší vejce.      1\n",
       "6       Včera večer jsme na procházce krmili labutě.      1\n",
       "7                             Nasadil jsem si brýle.      0\n",
       "8                           Večeřeli jsme v kuchyni.      0\n",
       "9                       Kohout ráno všechny vzbudil.      1"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data into pandas\n",
    "\n",
    "# df = pd.read_csv('data/sample_data.csv')\n",
    "# df_train, df_test = train_test_split(df, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "# df_train.to_csv('data/sample_data_train.csv', index=False)\n",
    "# df_test.to_csv('data/sample_data_test.csv', index=False)\n",
    "\n",
    "df_train = pd.read_csv('data/sample_data_train.csv')\n",
    "df_test = pd.read_csv('data/sample_data_test.csv')\n",
    "\n",
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into hugging_face dataset object and cast label as target\n",
    "\n",
    "ds = load_dataset('csv', data_files={'train': 'data/sample_data_train.csv', 'test': 'data/sample_data_test.csv'})\n",
    "label_features = ClassLabel(names=[\"není zvíře\", \"je zvíře\"])\n",
    "\n",
    "ds['train'] = ds['train'].cast_column(\"label\", label_features)\n",
    "ds['test'] = ds['test'].cast_column(\"label\", label_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb62223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['není zvíře', 'je zvíře'], id=None)}\n",
      "{'text': 'V zoologické zahradě měli lachtana.', 'label': 1}\n",
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['není zvíře', 'je zvíře'], id=None)}\n",
      "{'text': 'Pohladil jsem kočku.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "# examine values\n",
    "\n",
    "print(ds['train'].features)\n",
    "print(ds['train'][0])\n",
    "print(ds['test'].features)\n",
    "print(ds['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d7de4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = ds.features.copy()\n",
    "# features[\"label\"] = ClassLabel(names=[\"není zvíře\", \"je zvíře\"])\n",
    "# def adjust_labels(batch):\n",
    "# #     batch[\"label\"] = [sentiment + 1 for sentiment in batch[\"label\"]]\n",
    "#     return batch\n",
    "# ds = ds.map(adjust_labels, batched=True, features=features)\n",
    "# ds\n",
    "# ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efde4626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V zoologické zahradě měli lachtana.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Když budu celý život tvrdě pracovat, tak si st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zrzavá liška běžela v lese.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dneska jsem viděl jak soused kácel strom.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matematika je lepší než zeměpis.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                V zoologické zahradě měli lachtana.      1\n",
       "1  Když budu celý život tvrdě pracovat, tak si st...      0\n",
       "2                        Zrzavá liška běžela v lese.      1\n",
       "3          Dneska jsem viděl jak soused kácel strom.      0\n",
       "4                   Matematika je lepší než zeměpis.      0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display dataset as pandas dataframe\n",
    "\n",
    "ds['train'].set_format(type=\"pandas\")\n",
    "df = ds['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a696321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V zoologické zahradě měli lachtana.</td>\n",
       "      <td>1</td>\n",
       "      <td>je zvíře</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Když budu celý život tvrdě pracovat, tak si st...</td>\n",
       "      <td>0</td>\n",
       "      <td>není zvíře</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Zrzavá liška běžela v lese.</td>\n",
       "      <td>1</td>\n",
       "      <td>je zvíře</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dneska jsem viděl jak soused kácel strom.</td>\n",
       "      <td>0</td>\n",
       "      <td>není zvíře</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Matematika je lepší než zeměpis.</td>\n",
       "      <td>0</td>\n",
       "      <td>není zvíře</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  label_name\n",
       "0                V zoologické zahradě měli lachtana.      1    je zvíře\n",
       "1  Když budu celý život tvrdě pracovat, tak si st...      0  není zvíře\n",
       "2                        Zrzavá liška běžela v lese.      1    je zvíře\n",
       "3          Dneska jsem viděl jak soused kácel strom.      0  není zvíře\n",
       "4                   Matematika je lepší než zeměpis.      0  není zvíře"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map labels to label name\n",
    "\n",
    "def label_int2str(row):\n",
    "    return ds[\"train\"].features[\"label\"].int2str(row)\n",
    "\n",
    "df['label_name'] = df[\"label\"].apply(label_int2str)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ccbac6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying class frequencies\n",
    "\n",
    "df[\"label_name\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25cfd0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiYAAAGqCAYAAAA/VI94AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAm+0lEQVR4nO3de1SVdb7H8c8GYYMI2BReKAR1NJB0EjWvaKnpmJqXKZceSdTxksfRzMyyOaZYStpkOnk06jjaSq3R1mStTJ0c77cyNccKBDXURoumo2wN2iT8zh8t92nHRbfcfsn7tRYr98PveZ4vrkHe8+xnbxzGGCMAAAAL+FX3AAAAAFcQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAvGRnZ8vhcGjlypXVPQqAGogwAXBDW7NmjRYtWlTp5/n88881e/ZsZWdnV/q5gBsZYQLghlaVYZKSkkKYAOVEmAA1UF5eXnWPUGGMMcrPz6/uMQBUEMIEqEb//Oc/5XA49O6773q2HTx4UA6HQwkJCV5r+/Tpo/bt23ttW7p0qeLj4+V0OhUZGamJEyfqwoULXmvuvvtu3XHHHTp48KC6du2q2rVr66mnnpIkXbhwQSNHjlR4eLjq1q2r5OTkYvtL0ldffaVRo0bptttuk9PpVMOGDTVgwICrXh0YOXKk6tSpo5MnT6p3794KCQlRZGSk5syZo5//YvOioiItWrRI8fHxCgoKUv369TV+/HidP3/ea11MTIz69eunzZs3q23btgoODlZaWlqJ57/77ru1YcMGnTp1Sg6HQw6HQzExMZ7Pu91uzZo1S7/+9a/ldDoVFRWl6dOny+12e9YkJycrKChI6enpXsfu3bu3brrpJp09e1YrV67Ugw8+KEm65557POfavn17mX8/AIqrVd0DADXZHXfcobp162rnzp26//77JUm7du2Sn5+fjhw5IpfLpbCwMBUVFWnv3r0aN26cZ9/Zs2crJSVFPXv21IQJE3Ts2DEtW7ZMBw4c0J49exQQEOBZ++2336pPnz4aOnSokpKSVL9+fRljNGDAAO3evVsPP/yw4uLi9Pbbbys5ObnYnL/73e/02WefadKkSYqJiVFOTo4++OADnT592usHfUkKCwv129/+Vh06dNCCBQu0adMmzZo1S5cvX9acOXM868aPH6+VK1dq1KhRmjx5sr744gstWbJEhw8fLvb1HDt2TMOGDdP48eM1duxY3X777SWe+49//KNyc3P15Zdf6sUXX5Qk1alTR9KPIXT//fdr9+7dGjdunOLi4nT06FG9+OKLyszM1Pr16yVJixcv1tatW5WcnKx9+/bJ399faWlp+vvf/67XX39dkZGR6tq1qyZPnqw///nPeuqppxQXFydJnv8C8IEBUK369u1r7rrrLs/jwYMHm8GDBxt/f3+zceNGY4wxhw4dMpLMO++8Y4wxJicnxwQGBppevXqZwsJCz75Lliwxksxf/vIXz7Zu3boZSebll1/2Ou/69euNJLNgwQLPtsuXL5vExEQjyaxYscIYY8z58+eNJPP888/7/LUlJycbSWbSpEmebUVFRaZv374mMDDQfPPNN8YYY3bt2mUkmdWrV3vtv2nTpmLbo6OjjSSzadOma5qhb9++Jjo6utj2119/3fj5+Zldu3Z5bX/55ZeNJLNnzx7Pts2bNxtJ5tlnnzUnT540derUMQMHDvTab926dUaS2bZt2zXNBaBkPJUDVLPExEQdOnRI3333nSRp9+7duu+++3TnnXdq165dkn68iuJwONSlSxdJ0pYtW1RQUKApU6bIz+//v43Hjh2rsLAwbdiwwescTqdTo0aN8tr2/vvvq1atWpowYYJnm7+/vyZNmuS1Ljg4WIGBgdq+fXuxp1Wu1R/+8AfPnx0Oh/7whz+ooKBAW7ZskSStW7dO4eHhuvfee/Xvf//b89GmTRvVqVNH27Zt8zpe48aN1bt37+ua5Yp169YpLi5OsbGxXufs3r27JHmds1evXho/frzmzJmjwYMHKygoqNSnjwCUD0/lANUsMTFRly9f1r59+xQVFaWcnBwlJibqs88+8wqTFi1a6Fe/+pUk6dSpU5JU7CmMwMBANWnSxPP5K2699VYFBgZ6bTt16pQaNmzoeWrjip8f0+l0av78+XrsscdUv359dejQQf369dOIESPUoEGDq359fn5+atKkide25s2bS5LnHpWsrCzl5uaqXr16JR4jJyfH63Hjxo2vet6rycrKUnp6uiIiIq7pnH/605/0zjvv6JNPPtGaNWtKnRVA+RAmQDVr27atgoKCtHPnTjVq1Ej16tVT8+bNlZiYqKVLl8rtdmvXrl0aNGjQdZ8jODi4XDNOmTJF/fv31/r167V582bNnDlTqamp2rp1q1q3bl2uY0s/3u9Rr149rV69usTP/zweyvv1XDlny5YttXDhwhI/HxUV5fX48OHDnlg5evSohg0bVu4ZABRHmADVLDAwUHfddZd27dqlRo0aKTExUdKPV1LcbrdWr16tr7/+Wl27dvXsEx0dLenHm0B/ejWioKBAX3zxhXr27HnV80ZHR+sf//iHLl265HXV5NixYyWub9q0qR577DE99thjysrK0p133qkXXnhBq1atKvM8RUVFOnnypOcqiSRlZmZKkufG2aZNm2rLli3q3LlzhUTHTzkcjhK3N23aVEeOHFGPHj1KXXPFd999p1GjRqlFixbq1KmTFixYoEGDBqldu3ZXPQ8A33CPCWCBxMREffjhh9q2bZsnTG655RbFxcVp/vz5njVX9OzZU4GBgfrzn//s9bLb5cuXKzc3V3379r3qOe+77z5dvnxZy5Yt82wrLCzUSy+95LUuLy9P33//vde2pk2bKjQ01OtltWVZsmSJ58/GGC1ZskQBAQHq0aOHJGnIkCEqLCzUM888U2zfy5cvl/gS5msVEhKi3NzcYtuHDBmif/3rX3r11VeLfS4/P99zz48kPfHEEzp9+rRee+01LVy4UDExMUpOTvb6+kNCQiSpXLMC4IoJYIXExETNnTtXZ86c8QqQrl27Ki0tTTExMbrttts82yMiIjRjxgylpKTot7/9re6//34dO3ZMS5cuVbt27ZSUlHTVc/bv31+dO3fWk08+qezsbLVo0UJ/+9vfiv0Qz8zMVI8ePTRkyBC1aNFCtWrV0ttvv62vv/5aQ4cOvep5goKCtGnTJiUnJ6t9+/bauHGjNmzYoKeeesrzFE23bt00fvx4paam6pNPPlGvXr0UEBCgrKwsrVu3TosXL9YDDzxwrX+dXtq0aaO//vWvmjp1qtq1a6c6deqof//+euihh7R27Vo9/PDD2rZtmzp37qzCwkJlZGRo7dq1nvdJ2bp1q5YuXapZs2Z53ltmxYoVuvvuuzVz5kwtWLBAknTnnXfK399f8+fPV25urpxOp7p37869KICvqvtlQQCMcblcxt/f34SGhprLly97tq9atcpIMg899FCJ+y1ZssTExsaagIAAU79+fTNhwgRz/vx5rzXdunUz8fHxJe7/7bffmoceesiEhYWZ8PBw89BDD5nDhw97vVz43//+t5k4caKJjY01ISEhJjw83LRv396sXbv2ql9XcnKyCQkJMSdOnDC9evUytWvXNvXr1zezZs3yepnzFa+88opp06aNCQ4ONqGhoaZly5Zm+vTp5uzZs5410dHRpm/fvlc99xWXLl0y//Ef/2Hq1q1rJHm9dLigoMDMnz/fxMfHG6fTaW666SbTpk0bk5KSYnJzc43L5TLR0dEmISHB/PDDD17HffTRR42fn5/Zt2+fZ9urr75qmjRpYvz9/XnpMHCdHMb87O0XAaCCjBw5Um+99ZYuXbpU3aMA+IXgHhMAAGANwgQAAFiDMAEAANbgHhMAAGANrpgAAABrECYAAMAaVf4Ga0VFRTp79qxCQ0N5C2cAAGoIY4wuXryoyMhIr9+K/nNVHiZnz54t9suxAABAzXDmzBmvd7L+uSoPk9DQUEk/DhYWFlbVpwcAANXA5XIpKirK0wGlqfIwufL0TVhYGGECAEANc7XbOLj5FQAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFjDpzApLCzUzJkz1bhxYwUHB6tp06Z65plnZIyprPkAAEAN4tNb0s+fP1/Lli3Ta6+9pvj4eH388ccaNWqUwsPDNXny5MqaEQAA1BA+hcnevXs1YMAA9e3bV5IUExOjN954Qx999FGlDAcAAGoWn8KkU6dOeuWVV5SZmanmzZvryJEj2r17txYuXFjqPm63W2632/PY5XJd/7SwVl5enjIyMq66Lj8/X9nZ2YqJiVFwcHCZa2NjY1W7du2KGhEA8AvgU5g8+eSTcrlcio2Nlb+/vwoLCzV37lwNHz681H1SU1OVkpJS7kFht4yMDLVp06ZCj3nw4EElJCRU6DEBAHZzGB/uXH3zzTf1+OOP6/nnn1d8fLw++eQTTZkyRQsXLlRycnKJ+5R0xSQqKkq5ubkKCwsr/1cAK1zrFZP09HQlJSVp1apViouLK3MtV0wA4MbhcrkUHh5+1Z//Pl0xefzxx/Xkk09q6NChkqSWLVvq1KlTSk1NLTVMnE6nnE6nL6fBL1Dt2rV9uroRFxfH1RAAQDE+vVw4Ly9Pfn7eu/j7+6uoqKhChwIAADWTT1dM+vfvr7lz56pRo0aKj4/X4cOHtXDhQo0ePbqy5gMAADWIT2Hy0ksvaebMmfrP//xP5eTkKDIyUuPHj9fTTz9dWfMBAIAaxKcwCQ0N1aJFi7Ro0aJKGgcAANRk/K4cAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANbwKUxiYmLkcDiKfUycOLGy5gMAADVILV8WHzhwQIWFhZ7Hn376qe699149+OCDFT4YAACoeXwKk4iICK/Hzz33nJo2bapu3bpV6FAAAKBm8ilMfqqgoECrVq3S1KlT5XA4Sl3ndrvldrs9j10u1/WeEgBQxbKysnTx4sUy1+Tn5ys7O7tCzxsTE6Pg4OAy14SGhqpZs2YVel5Uv+sOk/Xr1+vChQsaOXJkmetSU1OVkpJyvacBAFSTrKwsNW/evLrHKFNmZiZxcoO57jBZvny5+vTpo8jIyDLXzZgxQ1OnTvU8drlcioqKut7TAgCqyJUrJatWrVJcXFyp66rjikl6erqSkpKuejUHvzzXFSanTp3Sli1b9Le//e2qa51Op5xO5/WcBgBggbi4OCUkJJS5pnPnzlU0DW501/U+JitWrFC9evXUt2/fip4HAADUYD6HSVFRkVasWKHk5GTVqnXdzwQBAAAU43OYbNmyRadPn9bo0aMrYx4AAFCD+XzJo1evXjLGVMYsAACghuN35QAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxRq7oHgP2ysrJ08eLFCjlWenq613/LKzQ0VM2aNauQYwEAqh9hgjJlZWWpefPmFX7cpKSkCjtWZmYmcQIANwjCBGW6cqVk1apViouLK/fx8vPzlZ2drZiYGAUHB5frWOnp6UpKSqqwqzkAgOpHmOCaxMXFKSEhoUKO1blz5wo5DgDgxsPNrwAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsIbPYfKvf/1LSUlJuvnmmxUcHKyWLVvq448/rozZAABADVPLl8Xnz59X586ddc8992jjxo2KiIhQVlaWbrrppsqaDwAA1CA+hcn8+fMVFRWlFStWeLY1bty4wocCAAA1k09P5bz77rtq27atHnzwQdWrV0+tW7fWq6++WuY+brdbLpfL6wMAAKAkPoXJyZMntWzZMjVr1kybN2/WhAkTNHnyZL322mul7pOamqrw8HDPR1RUVLmHBgAANyafwqSoqEgJCQmaN2+eWrdurXHjxmns2LF6+eWXS91nxowZys3N9XycOXOm3EMDAIAbk09h0rBhQ7Vo0cJrW1xcnE6fPl3qPk6nU2FhYV4fAAAAJfEpTDp37qxjx455bcvMzFR0dHSFDgUAAGomn8Lk0Ucf1f79+zVv3jwdP35ca9as0SuvvKKJEydW1nwAAKAG8SlM2rVrp7fffltvvPGG7rjjDj3zzDNatGiRhg8fXlnzAQCAGsSn9zGRpH79+qlfv36VMQsAAKjh+F05AADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKxBmAAAAGsQJgAAwBqECQAAsAZhAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALAGYQIAAKzhU5jMnj1bDofD6yM2NrayZgMAADVMLV93iI+P15YtW/7/ALV8PgQAAECJfK6KWrVqqUGDBpUxCwAAqOF8DpOsrCxFRkYqKChIHTt2VGpqqho1alTqerfbLbfb7Xnscrmub1JUi/z8fElSenp6NU9S3JWZrswIoGLx/Y/q4FOYtG/fXitXrtTtt9+uc+fOKSUlRYmJifr0008VGhpa4j6pqalKSUmpkGFR9bKzsyVJSUlJ1TtIGbKzs9W5c+fqHgO44fD9j+rgU5j06dPH8+dWrVqpffv2io6O1tq1a/X73/++xH1mzJihqVOneh67XC5FRUVd57ioajExMZKkVatWKS4urnqH+Zn09HQlJSV5ZgRQsfj+R3Uo152rdevWVfPmzXX8+PFS1zidTjmdzvKcBtUoODhYkhQXF6eEhIRqnqZkV2YEULH4/kd1KNf7mFy6dEknTpxQw4YNK2oeAABQg/kUJtOmTdOOHTuUnZ2tvXv3atCgQfL399ewYcMqaz4AAFCD+PRUzpdffqlhw4bp22+/VUREhLp06aL9+/crIiKisuYDAAA1iE9h8uabb1bWHAAAAPyuHAAAYA/CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgjVrVPQDslpeXJ0k6dOhQhRwvPz9f2dnZiomJUXBwcLmOlZ6eXiEzAQDsQZigTBkZGZKksWPHVvMkpQsNDa3uEQAAFYQwQZkGDhwoSYqNjVXt2rXLfbz09HQlJSVp1apViouLK/fxQkND1axZs3IfBwBgB8IEZbrllls0ZsyYCj9uXFycEhISKvy4AIBfNm5+BQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYoV5g899xzcjgcmjJlSgWNAwAAarLrDpMDBw4oLS1NrVq1qsh5AABADXZdYXLp0iUNHz5cr776qm666aaKngkAANRQ1/VL/CZOnKi+ffuqZ8+eevbZZ8tc63a75Xa7PY9dLtf1nBIAUMXy8vIkSYcOHSpzXX5+vrKzsyv03DExMQoODi718+np6RV6PtjD5zB58803dejQIR04cOCa1qempiolJcXnwQAA1SsjI0OSNHbs2GqepHShoaHVPQIqmE9hcubMGT3yyCP64IMPFBQUdE37zJgxQ1OnTvU8drlcioqK8m1KAECVGzhwoCQpNjZWtWvXLnVddVwxkX6MkmbNmlXoeVH9fAqTgwcPKicnRwkJCZ5thYWF2rlzp5YsWSK32y1/f3+vfZxOp5xOZ8VMCwCoMrfccovGjBlzTWs7d+5cydOgpvApTHr06KGjR496bRs1apRiY2P1xBNPFIsSAAAAX/gUJqGhobrjjju8toWEhOjmm28uth0AAMBXvPMrAACwxnW9XPintm/fXgFjAAAAcMUEAABYhDABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1iBMAACANQgTAABgDcIEAABYgzABAADWIEwAAIA1CBMAAGANwgQAAFiDMAEAANbwKUyWLVumVq1aKSwsTGFhYerYsaM2btxYWbMBAIAaxqcwue222/Tcc8/p4MGD+vjjj9W9e3cNGDBAn332WWXNBwAAapBavizu37+/1+O5c+dq2bJl2r9/v+Lj4yt0MAAAUPP4FCY/VVhYqHXr1um7775Tx44dS13ndrvldrs9j10u1/WeEhbLy8tTRkbGVdelp6d7/bcssbGxql27drlnAwD8cvgcJkePHlXHjh31/fffq06dOnr77bfVokWLUtenpqYqJSWlXEPCfhkZGWrTps01r09KSrrqmoMHDyohIaE8YwEAfmEcxhjjyw4FBQU6ffq0cnNz9dZbb+l//ud/tGPHjlLjpKQrJlFRUcrNzVVYWFj5poc1rvWKSX5+vrKzsxUTE6Pg4OAy13LFBABuHC6XS+Hh4Vf9+e9zmPxcz5491bRpU6WlpVXoYAAA4MZxrT//y/0+JkVFRV5XRAAAAK6XT/eYzJgxQ3369FGjRo108eJFrVmzRtu3b9fmzZsraz4AAFCD+BQmOTk5GjFihM6dO6fw8HC1atVKmzdv1r333ltZ8wEAgBrEpzBZvnx5Zc0BAADA78oBAAD2IEwAAIA1CBMAAGANwgQAAFiDMAEAANYgTAAAgDUIEwAAYA3CBAAAWIMwAQAA1vDpnV8rwpVfZuxyuar61AAAoJpc+bl/pQNKU+VhcvHiRUlSVFRUVZ8aAABUs4sXLyo8PLzUzzvM1dKlghUVFens2bMKDQ2Vw+GoylPDAi6XS1FRUTpz5ozCwsKqexwAVYjv/5rNGKOLFy8qMjJSfn6l30lS5VdM/Pz8dNttt1X1aWGZsLAw/mECaii+/2uusq6UXMHNrwAAwBqECQAAsAZhgirldDo1a9YsOZ3O6h4FQBXj+x/XospvfgUAACgNV0wAAIA1CBMAAGANwgQAUOGOHTum1NRUud3uSjn+hx9+qMWLF8sYo/fff18fffSRJGnr1q3av39/pZwTVYMwgZXS0tK0bds2FRYW6k9/+pPy8/NljNHixYuVl5dX3eMBKMPFixc1aNAgNW7cuFJudP3yyy81ZMgQtW7dWg6HQ/Hx8RoxYoT279+vyZMnq0WLFhV+TlSdKn+DNdhv5MiRunDhgtavX18t53/llVe0fPlybdu2Tf7+/vLz89OkSZMUGxur06dPq3bt2tUyF4Brk5ycrDFjxmjo0KEVfuyCggINGTJEL7zwgrp27SpJio6O1uOPP64uXbrovffe483bfuF4VQ6Kyc3NlTFGdevWrfJzf/TRRxo1apS2bdumevXqSfrx1xjcc889OnfunA4fPqyQkJAqnwvAL0dBQYECAwOrewxcJ57KQTHh4eHVEiWSdNddd+mzzz7zRIn0468x2LFjhzIzMxUUFKTCwsJqmQ34pbv77rs1efJkTZ8+Xb/61a/UoEEDzZ4922vNhQsXNGbMGEVERCgsLEzdu3fXkSNHPJ+fPXu27rzzTr3++uuKiYlReHi4hg4d6vkFrVfOM2XKlFLniImJkcPhKPYhSZ06ddITTzzhtf6bb75RQECAdu7c6dl/0aJFns87HA4tW7ZM999/v0JCQjRv3jxJ0jvvvKOEhAQFBQWpSZMmSklJ0eXLl6/nrw5ViDBBMSNHjtTAgQMl/Xi1IjU1VY0bN1ZwcLB+85vf6K233ip13+3bt5f4D87IkSOVmZkph8OhjIwMr31efPFFNW3a1Gv/CxcuSJJWrlypunXr6t1331WLFi3kdDp15swZud1uTZs2TbfeeqtCQkLUvn17bd++vTL+OoAbymuvvaaQkBB9+OGHWrBggebMmaMPPvjA8/kHH3xQOTk52rhxow4ePKiEhAT16NFD//u//+tZc+LECa1fv17vvfee3nvvPe3YsUPPPffcNc9w4MABnTt3TufOndOXX36pDh06KDExUZI0fPhwvfnmm/rpxfy//vWvioyM9KwpyezZszVo0CAdPXpUo0eP1q5duzRixAg98sgj+vzzz5WWlqaVK1dq7ty5vvx1oToY4GeSk5PNgAEDjDHGPPvssyY2NtZs2rTJnDhxwqxYscI4nU6zffv2Evd1u93m3Llzno+tW7eaoKAgs3z5cmOMMW3btjX/9V//5bVPmzZtPNu2bdtmJJnz588bY4xZsWKFCQgIMJ06dTJ79uwxGRkZ5rvvvjNjxowxnTp1Mjt37jTHjx83zz//vHE6nSYzM7Ny/lKAG0C3bt1Mly5dvLa1a9fOPPHEE8YYY3bt2mXCwsLM999/77WmadOmJi0tzRhjzKxZs0zt2rWNy+XyfP7xxx837du39zrPI488ck0zTZ482URHR5ucnBxjjDE5OTmmVq1aZufOnZ41HTt29MxojDHR0dHmxRdf9DyWZKZMmeJ13B49eph58+Z5bXv99ddNw4YNr2kuVB9ufkWp3G635s2bpy1btqhjx46SpCZNmmj37t1KS0tTt27diu0TGBioBg0aSJK+/fZbjRkzRqNHj9bo0aMl/fj/hpYsWaJnnnlGkpSZmamDBw9q1apVpc7xww8/aOnSpfrNb34jSTp9+rRWrFih06dPKzIyUpI0bdo0bdq0SStWrPBcxgVQXKtWrbweN2zYUDk5OZKkI0eO6NKlS7r55pu91uTn5+vEiROexzExMQoNDS3xGL64cqP73r17FRERIUmKiIhQr169tHr1aiUmJuqLL77Qvn37lJaWVuax2rZt6/X4yJEj2rNnj9cVksLCQn3//ffKy8vjJnqLESYo1fHjx5WXl6d7773Xa3tBQYFat25d5r4//PCDfve73yk6OlqLFy/2bB86dKimTZum/fv3q0OHDlq9erUSEhIUGxtb6rECAwO9/jE9evSoCgsL1bx5c691bre72D+oALwFBAR4PXY4HCoqKpIkXbp0SQ0bNizxadGf3ndW1jGu1bZt2zRp0iS98cYbxWJp+PDhmjx5sl566SWtWbNGLVu2VMuWLcs83s9vir906ZJSUlI0ePDgYmuDgoJ8mhVVizBBqS5duiRJ2rBhg2699Vavz13tvQkmTJigM2fO6KOPPlKtWv//P7MGDRqoe/fuWrNmjTp06KA1a9ZowoQJZR4rODjYc2Pclbn8/f118OBB+fv7e62tU6fONX1tAIpLSEjQV199pVq1aikmJqbSznP8+HE98MADeuqpp0oMhwEDBmjcuHHatGmT1qxZoxEjRvh8joSEBB07dky//vWvK2JkVCHCBKW6crPp6dOnS3zapjQLFy7U2rVrtXfv3hKvYAwfPlzTp0/XsGHDdPLkSZ/f66B169YqLCxUTk5OmTfDAfBNz5491bFjRw0cOFALFixQ8+bNdfbsWW3YsEGDBg0q9nTJ9cjPz1f//v3VunVrjRs3Tl999ZXnc1eeBg4JCdHAgQM1c+ZMpaena9iwYT6f5+mnn1a/fv3UqFEjPfDAA/Lz89ORI0f06aef6tlnny3314HKQ5igVKGhoZo2bZoeffRRFRUVqUuXLsrNzdWePXsUFham5OTkYvts2bJF06dP13//93/rlltu8fyjExwcrPDwcEnS4MGDNWHCBE2YMEH33HOP5z6Ra9W8eXMNHz5cI0aM0AsvvKDWrVvrm2++0T/+8Q+1atVKffv2Lf8XD9RADodD77//vv74xz9q1KhR+uabb9SgQQN17dpV9evXr5BzfP3118rIyFBGRkax733zk1fiDB8+XPfdd5+6du2qRo0a+Xye3r1767333tOcOXM0f/58BQQEKDY2VmPGjCn314BKVt1338I+P31VTlFRkVm0aJG5/fbbTUBAgImIiDC9e/c2O3bsKHHfWbNmGUnFPpKTk73WDRkyxEgyf/nLX7y2l/SqnPDw8GLnKSgoME8//bSJiYkxAQEBpmHDhmbQoEHmn//8Z3m/fABANeKdX1HMsGHD5O/vX+YrZQAAqAy8wRo8Ll++rM8//1z79u1TfHx8dY8DAKiBCBN4fPrpp2rbtq3i4+P18MMPV/c4AIAaiKdyAACANbhiAgAArEGYAAAAaxAmAADAGoQJAACwBmECAACsQZgAAABrECYAAMAahAkAALDG/wHRv7Z114/N+QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displaying boxplots for word frequencies per class\n",
    "\n",
    "df[\"words per text\"] = df[\"text\"].str.split().apply(len)\n",
    "df.boxplot(\"words per text\", by=\"label_name\", grid=False, showfliers=False,\n",
    "           color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46701c6",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a721ec77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[    2,    63,  7501,  5368,  6695,  1931, 16102,   154, 10097,\n",
       "          145,    18,     3,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,   593,  2631,  1912,  1764, 18973,  4844,    16,   316,\n",
       "          354,  1633,  6691, 22633, 13960,  1352,  1971,    18,     3],\n",
       "       [    2,  3596, 11583,   155, 20550, 11871,   246,    63,  9110,\n",
       "           18,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,  5462,   409,  6006,   334,  4777,  2049,  6976,  6305,\n",
       "           18,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 16537,   252,  2082,   688, 21387,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 22251, 14416,   300,  8036,    18,     3,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,  5353,  2525,   594,   242, 15029,   248,  7024,   249,\n",
       "        17998,   333,    18,     3,     0,     0,     0,     0,     0],\n",
       "       [    2, 20581,   327,   409,   354,  7733,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 11311,   249,   594,    63,  6970,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 18040,  3871,   876, 12616,  2852,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 15483,   246,   252, 23839,   302,  6299,    16,   737,\n",
       "         1630,  5739,  5959,   145,    18,     3,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "# tokenized_data_train = \n",
    "tokenizer(ds['train']['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# labels_train = np.array(ds['train']['label'])\n",
    "# tokenized_data_test = tokenizer(ds['test']['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "# labels_test = np.array(ds['test']['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0deabe92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[    2,    63,  7501,  5368,  6695,  1931, 16102,   154, 10097,\n",
       "          145,    18,     3,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,   593,  2631,  1912,  1764, 18973,  4844,    16,   316,\n",
       "          354,  1633,  6691, 22633, 13960,  1352,  1971,    18,     3],\n",
       "       [    2,  3596, 11583,   155, 20550, 11871,   246,    63,  9110,\n",
       "           18,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,  5462,   409,  6006,   334,  4777,  2049,  6976,  6305,\n",
       "           18,     3,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 16537,   252,  2082,   688, 21387,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 22251, 14416,   300,  8036,    18,     3,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2,  5353,  2525,   594,   242, 15029,   248,  7024,   249,\n",
       "        17998,   333,    18,     3,     0,     0,     0,     0,     0],\n",
       "       [    2, 20581,   327,   409,   354,  7733,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 11311,   249,   594,    63,  6970,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 18040,  3871,   876, 12616,  2852,    18,     3,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "       [    2, 15483,   246,   252, 23839,   302,  6299,    16,   737,\n",
       "         1630,  5739,  5959,   145,    18,     3,     0,     0,     0]])>, 'token_type_ids': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(11, 18), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]])>}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attention mask umožňuje ignorovat padding\n",
    "tokenized_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9e53118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'v',\n",
       " 'zoo',\n",
       " '##logické',\n",
       " 'zahradě',\n",
       " 'měli',\n",
       " 'lac',\n",
       " '##h',\n",
       " '##tan',\n",
       " '##a',\n",
       " '.',\n",
       " '[SEP]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]',\n",
       " '[PAD]']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# navrácení tokenů k id\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_data_train[0].ids)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b67520de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] v zoologické zahradě měli lachtana. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tisk jako string\n",
    "tokenizer.convert_tokens_to_string(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "143dc1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# počet tokenů ve slovníku\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e241886c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# maximální délka vektoru\n",
    "# (... mám o tom nějaké pochybnosti ....)\n",
    "\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "182704b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'token_type_ids', 'attention_mask']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9061fc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdba8a24edef4a1795c35127f73ae858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e091508c664f11836050a90e7e9f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 6\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizace celého datasetu\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
    "\n",
    "tokenized = ds.map(tokenize, batched=True, batch_size=None)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c0eef",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4bf44e99",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of 'ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    613\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\utils\\hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:172\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[1;34m(repo_id)\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    174\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    176\u001b[0m     )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m repo_id:\n",
      "\u001b[1;31mHFValidationError\u001b[0m: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m hub_kwargs \u001b[38;5;241m=\u001b[39m {name: kwargs\u001b[38;5;241m.\u001b[39mpop(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m hub_kwargs_names \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 434\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_unused_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:809\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    808\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 809\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[0;32m    558\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[1;32m--> 559\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[0;32m    561\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:635\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[1;32m--> 635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the configuration of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    637\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    638\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    639\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    640\u001b[0m         )\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    643\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[0;32m    644\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load the configuration of 'ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'ElectraModel(\n  (embeddings): ElectraEmbeddings(\n    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n    (position_embeddings): Embedding(512, 128)\n    (token_type_embeddings): Embedding(2, 128)\n    (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n  (encoder): ElectraEncoder(\n    (layer): ModuleList(\n      (0): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): ElectraLayer(\n        (attention): ElectraAttention(\n          (self): ElectraSelfAttention(\n            (query): Linear(in_features=256, out_features=256, bias=True)\n            (key): Linear(in_features=256, out_features=256, bias=True)\n            (value): Linear(in_features=256, out_features=256, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): ElectraSelfOutput(\n            (dense): Linear(in_features=256, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): ElectraIntermediate(\n          (dense): Linear(in_features=256, out_features=1024, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): ElectraOutput(\n          (dense): Linear(in_features=1024, out_features=256, bias=True)\n          (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AutoModel.from_pretrained(model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694aea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow is also available\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "tf_model = TFAutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "86f34f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor shape: torch.Size([1, 6])\n",
      "BaseModelOutputWithPastAndCrossAttentions(last_hidden_state=tensor([[[ 3.6099e-01,  5.5451e-05, -1.2910e-04,  ...,  2.7867e-04,\n",
      "           4.9307e-01, -1.3343e-04],\n",
      "         [-1.9553e-01, -3.0069e-04, -1.0528e-04,  ...,  9.9561e-05,\n",
      "          -1.1577e+00, -1.9735e-04],\n",
      "         [-4.4455e-01, -4.6349e-04, -9.3824e-05,  ...,  3.3250e-04,\n",
      "           1.7377e-01, -1.3628e-04],\n",
      "         [-2.3091e-02, -2.4707e-04, -7.6978e-05,  ...,  6.1547e-04,\n",
      "           6.3975e-01, -1.4361e-04],\n",
      "         [ 1.0250e+00, -2.7454e-04, -7.3393e-05,  ...,  7.0218e-04,\n",
      "           4.0733e-01, -2.5776e-04],\n",
      "         [ 3.6190e-01,  5.5545e-05, -1.2914e-04,  ...,  2.7877e-04,\n",
      "           4.9256e-01, -1.3352e-04]]]), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n",
      "torch.Size([1, 6, 256])\n"
     ]
    }
   ],
   "source": [
    "text = \"Toto je zkušební text\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n",
    "\n",
    "inputs = {k:v.to(device) for k,v in inputs.items()}\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)\n",
    "\n",
    "print(outputs.last_hidden_state.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74ca94",
   "metadata": {},
   "source": [
    "<i>\"Looking at the hidden state tensor, we see that it has the shape [batch_size, n_tokens, hidden_dim]. In other words, a 256-dimensional vector is returned for each of the 6 input tokens. For classification tasks, it is common practice to just use the hidden state associated with the [CLS] token as the input feature. Since this token appears at the start of each sequence, we can extract it by simply indexing into outputs.last_hidden_state as follows:\"</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "40af16db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:,0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eec1a837",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k:v.to(device) for k,v in batch.items() \n",
    "              if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state\n",
    "    # Return vector for [CLS] token\n",
    "    return {\"hidden_state\": last_hidden_state[:,0].cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffd6d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# token_type_ids?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ed14369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6ff8054fd14592a8b67063d17d18bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# hidden states\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m hidden \u001b[38;5;241m=\u001b[39m \u001b[43mtokenized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextract_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:816\u001b[0m, in \u001b[0;36mDatasetDict.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[1;32m--> 816\u001b[0m     {\n\u001b[0;32m    817\u001b[0m         k: dataset\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m    818\u001b[0m             function\u001b[38;5;241m=\u001b[39mfunction,\n\u001b[0;32m    819\u001b[0m             with_indices\u001b[38;5;241m=\u001b[39mwith_indices,\n\u001b[0;32m    820\u001b[0m             with_rank\u001b[38;5;241m=\u001b[39mwith_rank,\n\u001b[0;32m    821\u001b[0m             input_columns\u001b[38;5;241m=\u001b[39minput_columns,\n\u001b[0;32m    822\u001b[0m             batched\u001b[38;5;241m=\u001b[39mbatched,\n\u001b[0;32m    823\u001b[0m             batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m    824\u001b[0m             drop_last_batch\u001b[38;5;241m=\u001b[39mdrop_last_batch,\n\u001b[0;32m    825\u001b[0m             remove_columns\u001b[38;5;241m=\u001b[39mremove_columns,\n\u001b[0;32m    826\u001b[0m             keep_in_memory\u001b[38;5;241m=\u001b[39mkeep_in_memory,\n\u001b[0;32m    827\u001b[0m             load_from_cache_file\u001b[38;5;241m=\u001b[39mload_from_cache_file,\n\u001b[0;32m    828\u001b[0m             cache_file_name\u001b[38;5;241m=\u001b[39mcache_file_names[k],\n\u001b[0;32m    829\u001b[0m             writer_batch_size\u001b[38;5;241m=\u001b[39mwriter_batch_size,\n\u001b[0;32m    830\u001b[0m             features\u001b[38;5;241m=\u001b[39mfeatures,\n\u001b[0;32m    831\u001b[0m             disable_nullable\u001b[38;5;241m=\u001b[39mdisable_nullable,\n\u001b[0;32m    832\u001b[0m             fn_kwargs\u001b[38;5;241m=\u001b[39mfn_kwargs,\n\u001b[0;32m    833\u001b[0m             num_proc\u001b[38;5;241m=\u001b[39mnum_proc,\n\u001b[0;32m    834\u001b[0m             desc\u001b[38;5;241m=\u001b[39mdesc,\n\u001b[0;32m    835\u001b[0m         )\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\dataset_dict.py:817\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cache_file_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    814\u001b[0m     cache_file_names \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m}\n\u001b[0;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DatasetDict(\n\u001b[0;32m    816\u001b[0m     {\n\u001b[1;32m--> 817\u001b[0m         k: \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m            \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_names\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, dataset \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    837\u001b[0m     }\n\u001b[0;32m    838\u001b[0m )\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:2826\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2823\u001b[0m disable_tqdm \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mis_progress_bar_enabled()\n\u001b[0;32m   2825\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m num_proc \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2829\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_rank\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2830\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2832\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2833\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_last_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_last_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2834\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2835\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2836\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload_from_cache_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_from_cache_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2837\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_file_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2838\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_nullable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_nullable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2845\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2846\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2848\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mformat_cache_file_name\u001b[39m(cache_file_name, rank):\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:562\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    560\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    561\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 562\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    563\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:529\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    522\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    527\u001b[0m }\n\u001b[0;32m    528\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    530\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    531\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\fingerprint.py:480\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[1;32m--> 480\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3247\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   3243\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3244\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(input_dataset\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3245\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3247\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3251\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3253\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3256\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[1;32mc:\\users\\capek\\pycharm_projects\\transformers_for_czech_language\\venv\\lib\\site-packages\\datasets\\arrow_dataset.py:3123\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3122\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3123\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3125\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3126\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3127\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[81], line 3\u001b[0m, in \u001b[0;36mextract_hidden_states\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_hidden_states\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Place model inputs on the GPU\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k:v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \n\u001b[0;32m      4\u001b[0m               \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract last hidden states\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[1;32mIn[81], line 3\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_hidden_states\u001b[39m(batch):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Place model inputs on the GPU\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k:\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \n\u001b[0;32m      4\u001b[0m               \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mmodel_input_names}\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract last hidden states\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "# hidden states\n",
    "hidden = tokenized.map(extract_hidden_states, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d9693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = train_ds.to_tf_dataset(\n",
    "    columns=[\"text\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=4,\n",
    "    shuffle=True)\n",
    "\n",
    "tf_test = test_ds.to_tf_dataset(\n",
    "    columns=[\"text\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=4,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "590ccdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9290f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d1d21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3c4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_data_train = tokenizer(train_ds['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "labels_train = np.array(train_ds['target'])\n",
    "tokenized_data_test = tokenizer(test_ds['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "labels_test = np.array(test_ds['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f28d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[    2, 16102,   154, 10097,     3],\n",
       "       [    2,  1971,     3,     0,     0],\n",
       "       [    2, 20550,     3,     0,     0],\n",
       "       [    2,  6305,     3,     0,     0],\n",
       "       [    2, 16537,     3,     0,     0],\n",
       "       [    2, 22251,     3,     0,     0],\n",
       "       [    2, 17998,   187,     3,     0],\n",
       "       [    2,  7733,     3,     0,     0],\n",
       "       [    2,  3876,     3,     0,     0],\n",
       "       [    2, 18040,     3,     0,     0],\n",
       "       [    2, 15483,   246,     3,     0]])>, 'token_type_ids': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 0]])>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a2a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Seznam/small-e-czech were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Lower learning rates are often better for fine-tuning transformers\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m3e-5\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:117\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[1;34m(self, key, use_function_subtyping)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_function_subtyping:\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 117\u001b[0m dispatch_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[dispatch_key]\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\type_dispatch.py:78\u001b[0m, in \u001b[0;36mTypeDispatchTable.dispatch\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the deepest subtype target if it exists in the table.\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# For known exact matches.\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m:\n\u001b[0;32m     79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m request\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# For known non-exact matches.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# (self._dispatch cache does not contain exact matches)\u001b[39;00m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:77\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:246\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:106\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:584\u001b[0m, in \u001b[0;36mReference.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 584\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model)\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ec9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[0.05621602],\n",
       "       [0.05620134],\n",
       "       [0.06481404],\n",
       "       [0.05680045],\n",
       "       [0.0567313 ],\n",
       "       [0.05758224]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenized_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4b42520",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.text.apply(tokenize_text).values\n",
    "X_test = np.asarray(df_test.text.apply(tokenize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1e860d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([    2, 16102,   154, 10097,     3]),\n",
       "       array([   2, 1971,    3]), array([    2, 20550,     3]),\n",
       "       array([   2, 6305,    3]), array([    2, 16537,     3]),\n",
       "       array([    2, 22251,     3]), array([    2, 17998,   187,     3]),\n",
       "       array([   2, 7733,    3]), array([   2, 3876,    3]),\n",
       "       array([    2, 18040,     3]), array([    2, 15483,   246,     3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "028d23cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...classifier\n",
      "......vars\n",
      "...classifier\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...classifier\\dropout\n",
      "......vars\n",
      "...classifier\\out_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\n",
      "......vars\n",
      "...electra\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...electra\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\embeddings\\dropout\n",
      "......vars\n",
      "...electra\\embeddings_project\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n",
      "Keras model archive saving:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\capek\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\generation\\tf_utils.py:446: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-22 11:42:54         2143\n",
      "metadata.json                                  2022-12-22 11:42:54           64\n",
      "variables.h5                                   2022-12-22 11:42:54     54702448\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-22 11:42:54         2143\n",
      "metadata.json                                  2022-12-22 11:42:54           64\n",
      "variables.h5                                   2022-12-22 11:42:54     54702448\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...classifier\n",
      "......vars\n",
      "...classifier\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...classifier\\dropout\n",
      "......vars\n",
      "...classifier\\out_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\n",
      "......vars\n",
      "...electra\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...electra\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\embeddings\\dropout\n",
      "......vars\n",
      "...electra\\embeddings_project\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Failed to delete a file: C:\\Users\\capek\\AppData\\Local\\Temp\\tmp2agw6zos/variables.h5; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:187\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    186\u001b[0m assets_dir \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(temp_path, _ASSETS_DIRNAME)\n\u001b[1;32m--> 187\u001b[0m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH5IOHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDiskIOHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43massets_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m h5_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:280\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_keras_trackable(child_obj):\n\u001b[1;32m--> 280\u001b[0m     \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:280\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_keras_trackable(child_obj):\n\u001b[1;32m--> 280\u001b[0m     \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:264\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_own_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 264\u001b[0m     \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_own_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:3439\u001b[0m, in \u001b[0;36mLayer._load_own_variables\u001b[1;34m(self, store)\u001b[0m\n\u001b[0;32m   3438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[1;32m-> 3439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames of variables received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3444\u001b[0m     )\n\u001b[0;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[0;32m   3446\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'dense' expected 0 variables, but received 2 variables during loading. Names of variables received: ['0', '1']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFAutoModelForSequenceClassification\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m hub_kwargs \u001b[38;5;241m=\u001b[39m {name: kwargs\u001b[38;5;241m.\u001b[39mpop(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m hub_kwargs_names \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 434\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    435\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    436\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:809\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    808\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 809\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:557\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config_dict\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    [`PretrainedConfig`] using `from_dict`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     original_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\engine\\training.py:381\u001b[0m, in \u001b[0;36mModel.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__deepcopy__\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m--> 381\u001b[0m         new \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_model_from_bytecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_model_as_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m         memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;66;03m# See comment in __reduce__ for explanation\u001b[39;00m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_lib\u001b[38;5;241m.\u001b[39mload_model(filepath)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     40\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(serialized_model)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# a custom_object_scope.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:202\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    200\u001b[0m _SAVING_V3_ENABLED\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m saving_v3_enabled_value\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(temp_path):\n\u001b[1;32m--> 202\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:676\u001b[0m, in \u001b[0;36mdelete_recursively_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.rmtree\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_recursively_v2\u001b[39m(path):\n\u001b[0;32m    668\u001b[0m   \u001b[38;5;124;03m\"\"\"Deletes everything under path recursively.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 676\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeleteRecursively\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Failed to delete a file: C:\\Users\\capek\\AppData\\Local\\Temp\\tmp2agw6zos/variables.h5; Permission denied"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c6261d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m3e-5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(np.array(df.text.apply(tokenize_text)), np.array(df.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "47756271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kočka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>velbloud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>atom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>počítač</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kalkulačka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  target\n",
       "0        Kočka       1\n",
       "1          pes       1\n",
       "5     velbloud       1\n",
       "15        atom       0\n",
       "11     počítač       0\n",
       "14  kalkulačka       0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa533509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
