{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401f42e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Seznam/small-e-czech were not used when initializing ElectraForSequenceClassification: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraForPreTraining, ElectraTokenizerFast\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model = \"Seznam/small-e-czech\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipe = pipeline(task=\"text-classification\", model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c15dc24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lachtan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dům</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liška</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matematika</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  label\n",
       "0     lachtan      1\n",
       "1         dům      0\n",
       "2       liška      1\n",
       "3       strom      0\n",
       "4  matematika      0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = pd.read_csv('data/sample_data.csv')\n",
    "# df_train, df_test = train_test_split(df, test_size=0.33, random_state=42, shuffle=True)\n",
    "\n",
    "# df_train.to_csv('data/sample_data_train.csv', index=False)\n",
    "# df_test.to_csv('data/sample_data_test.csv', index=False)\n",
    "\n",
    "df_train = pd.read_csv('data/sample_data_train.csv')\n",
    "df_test = pd.read_csv('data/sample_data_test.csv')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "70d9a719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f8ec23a20551b00e\n",
      "Found cached dataset csv (C:/Users/capek/.cache/huggingface/datasets/csv/default-f8ec23a20551b00e/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e417ae0cb9dc49c08f3477c729aa78f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\capek\\.cache\\huggingface\\datasets\\csv\\default-f8ec23a20551b00e\\0.0.0\\6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\\cache-ea30b3ae017bf277.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, ClassLabel\n",
    "import nlp\n",
    "ds = load_dataset('csv', data_files={'train': 'data/sample_data_train.csv', 'test': 'data/sample_data_test.csv'})\n",
    "label_features = ClassLabel(names=[\"není zvíře\", \"je zvíře\"])\n",
    "train_ds = ds['train']\n",
    "# train_ds['label'] = label_features\n",
    "train_ds.cast_column(\"label\", label_features)\n",
    "test_ds = ds['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1cb62223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = train_ds\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26d7de4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86102082263f4d3b8d7dba4867511be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 11\n",
       "})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ds.features.copy()\n",
    "features[\"label\"] = ClassLabel(names=[\"není zvíře\", \"je zvíře\"])\n",
    "def adjust_labels(batch):\n",
    "#     batch[\"label\"] = [sentiment + 1 for sentiment in batch[\"label\"]]\n",
    "    return batch\n",
    "ds = ds.map(adjust_labels, batched=True, features=features)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3c768c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(names=['není zvíře', 'je zvíře'], id=None)}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2bb0f91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['není zvíře', 'je zvíře'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d2e4cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds.features[\"basic_sentiment\"] = ClassLabel(names=[\"negative\", \"neutral\", \"positive\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "90659aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "73b8ad4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'lachtan', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "efde4626",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lachtan</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dům</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>liška</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>strom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>matematika</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  label\n",
       "0     lachtan      1\n",
       "1         dům      0\n",
       "2       liška      1\n",
       "3       strom      0\n",
       "4  matematika      0"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ds.set_format(type=\"pandas\")\n",
    "df = ds[:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a696321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbac6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cfd0a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61d9693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train = ds['train'].to_tf_dataset(\n",
    "    columns=[\"text\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=4,\n",
    "    shuffle=True)\n",
    "\n",
    "tf_test = ds['test'].to_tf_dataset(\n",
    "    columns=[\"text\"],\n",
    "    label_cols=[\"label\"],\n",
    "    batch_size=4,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "590ccdd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9290f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Unnamed: 0': Value(dtype='int64', id=None),\n",
       " 'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8d1d21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': Value(dtype='int64', id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3c4f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_data_train = tokenizer(train_ds['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "labels_train = np.array(train_ds['target'])\n",
    "tokenized_data_test = tokenizer(test_ds['text'], return_tensors=\"tf\", padding=True, truncation=True)\n",
    "labels_test = np.array(test_ds['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16f28d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[    2, 16102,   154, 10097,     3],\n",
       "       [    2,  1971,     3,     0,     0],\n",
       "       [    2, 20550,     3,     0,     0],\n",
       "       [    2,  6305,     3,     0,     0],\n",
       "       [    2, 16537,     3,     0,     0],\n",
       "       [    2, 22251,     3,     0,     0],\n",
       "       [    2, 17998,   187,     3,     0],\n",
       "       [    2,  7733,     3,     0,     0],\n",
       "       [    2,  3876,     3,     0,     0],\n",
       "       [    2, 18040,     3,     0,     0],\n",
       "       [    2, 15483,   246,     3,     0]])>, 'token_type_ids': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0]])>, 'attention_mask': <tf.Tensor: shape=(11, 5), dtype=int32, numpy=\n",
       "array([[1, 1, 1, 1, 1],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 0, 0],\n",
       "       [1, 1, 1, 1, 0]])>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a2a6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at Seznam/small-e-czech were not used when initializing TFElectraForSequenceClassification: ['discriminator_predictions']\n",
      "- This IS expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFElectraForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFElectraForSequenceClassification were not initialized from the model checkpoint at Seznam/small-e-czech and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Lower learning rates are often better for fine-tuning transformers\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m3e-5\u001b[39m))\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_data_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:117\u001b[0m, in \u001b[0;36mFunctionCache.lookup\u001b[1;34m(self, key, use_function_subtyping)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_function_subtyping:\n\u001b[0;32m    115\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary\u001b[38;5;241m.\u001b[39mget(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 117\u001b[0m dispatch_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dispatch_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    119\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_primary[dispatch_key]\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\type_dispatch.py:78\u001b[0m, in \u001b[0;36mTypeDispatchTable.dispatch\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the deepest subtype target if it exists in the table.\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# For known exact matches.\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_table\u001b[49m:\n\u001b[0;32m     79\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m request\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# For known non-exact matches.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# (self._dispatch cache does not contain exact matches)\u001b[39;00m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_cache.py:77\u001b[0m, in \u001b[0;36mFunctionCacheKey.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m---> 77\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:246\u001b[0m, in \u001b[0;36mFunctionType.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 246\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:106\u001b[0m, in \u001b[0;36mParameter.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 106\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:207\u001b[0m, in \u001b[0;36mTuple.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 207\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomponents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\core\\function\\trace_type\\default_types.py:584\u001b[0m, in \u001b[0;36mReference.__hash__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__hash__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m--> 584\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mhash\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot generate a hashable key for IteratorSpec(({'input_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'token_type_ids': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(None, 5), dtype=tf.int32, name=None)}, TensorSpec(shape=(None,), dtype=tf.int32, name=None)),) because the _serialize() method returned an unsupproted value of type <class 'transformers.tokenization_utils_base.BatchEncoding'>"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model)\n",
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "\n",
    "model.fit(tokenized_data_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ec9d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[0.05621602],\n",
       "       [0.05620134],\n",
       "       [0.06481404],\n",
       "       [0.05680045],\n",
       "       [0.0567313 ],\n",
       "       [0.05758224]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(tokenized_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c4b42520",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.text.apply(tokenize_text).values\n",
    "X_test = np.asarray(df_test.text.apply(tokenize_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1e860d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([    2, 16102,   154, 10097,     3]),\n",
       "       array([   2, 1971,    3]), array([    2, 20550,     3]),\n",
       "       array([   2, 6305,    3]), array([    2, 16537,     3]),\n",
       "       array([    2, 22251,     3]), array([    2, 17998,   187,     3]),\n",
       "       array([   2, 7733,    3]), array([   2, 3876,    3]),\n",
       "       array([    2, 18040,     3]), array([    2, 15483,   246,     3])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "028d23cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...classifier\n",
      "......vars\n",
      "...classifier\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...classifier\\dropout\n",
      "......vars\n",
      "...classifier\\out_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\n",
      "......vars\n",
      "...electra\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...electra\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\embeddings\\dropout\n",
      "......vars\n",
      "...electra\\embeddings_project\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n",
      "Keras model archive saving:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\capek\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\generation\\tf_utils.py:446: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-22 11:42:54         2143\n",
      "metadata.json                                  2022-12-22 11:42:54           64\n",
      "variables.h5                                   2022-12-22 11:42:54     54702448\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2022-12-22 11:42:54         2143\n",
      "metadata.json                                  2022-12-22 11:42:54           64\n",
      "variables.h5                                   2022-12-22 11:42:54     54702448\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...classifier\n",
      "......vars\n",
      "...classifier\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...classifier\\dropout\n",
      "......vars\n",
      "...classifier\\out_proj\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\n",
      "......vars\n",
      "...electra\\embeddings\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........2\n",
      "...electra\\embeddings\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\embeddings\\dropout\n",
      "......vars\n",
      "...electra\\embeddings_project\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_10\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_11\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_1\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_2\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_3\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_4\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_5\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_6\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_7\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_8\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\dense_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\key\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\query\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\attention\\self_attention\\value\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\LayerNorm\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\bert_output\\dropout\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\n",
      "......vars\n",
      "...electra\\encoder\\layer\\tf_electra_layer_9\\intermediate\\dense\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      "...vars\n"
     ]
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Failed to delete a file: C:\\Users\\capek\\AppData\\Local\\Temp\\tmp2agw6zos/variables.h5; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:196\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:187\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    186\u001b[0m assets_dir \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mjoin(temp_path, _ASSETS_DIRNAME)\n\u001b[1;32m--> 187\u001b[0m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH5IOHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh5_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDiskIOHandler\u001b[49m\u001b[43m(\u001b[49m\u001b[43massets_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m h5_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:280\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_keras_trackable(child_obj):\n\u001b[1;32m--> 280\u001b[0m     \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:280\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_keras_trackable(child_obj):\n\u001b[1;32m--> 280\u001b[0m     \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchild_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m        \u001b[49m\u001b[43massets_handler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43minner_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchild_attr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvisited_trackables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisited_trackables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(child_obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:264\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(trackable, weights_handler, assets_handler, inner_path, visited_trackables)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_own_variables\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 264\u001b[0m     \u001b[43mtrackable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_own_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trackable, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_load_assets\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\engine\\base_layer.py:3439\u001b[0m, in \u001b[0;36mLayer._load_own_variables\u001b[1;34m(self, store)\u001b[0m\n\u001b[0;32m   3438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_vars):\n\u001b[1;32m-> 3439\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3440\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_vars)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3442\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m variables during loading. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNames of variables received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(store\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3444\u001b[0m     )\n\u001b[0;32m   3445\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(all_vars):\n\u001b[0;32m   3446\u001b[0m     \u001b[38;5;66;03m# TODO(rchao): check shapes and raise errors.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer 'dense' expected 0 variables, but received 2 variables during loading. Names of variables received: ['0', '1']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[128], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFAutoModelForSequenceClassification\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:434\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    432\u001b[0m hub_kwargs \u001b[38;5;241m=\u001b[39m {name: kwargs\u001b[38;5;241m.\u001b[39mpop(name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m hub_kwargs_names \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m kwargs}\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m--> 434\u001b[0m     config, kwargs \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    435\u001b[0m         pretrained_model_name_or_path,\n\u001b[0;32m    436\u001b[0m         return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    437\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    440\u001b[0m     )\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:809\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    808\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 809\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\transformers\\configuration_utils.py:557\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config_dict\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    544\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    545\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    From a `pretrained_model_name_or_path`, resolve to a dictionary of parameters, to be used for instantiating a\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    [`PretrainedConfig`] using `from_dict`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    555\u001b[0m \n\u001b[0;32m    556\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 557\u001b[0m     original_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m    559\u001b[0m     config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    144\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:231\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[1;34m(x, memo, deepcopy)\u001b[0m\n\u001b[0;32m    229\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m--> 231\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\copy.py:153\u001b[0m, in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    151\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\engine\\training.py:381\u001b[0m, in \u001b[0;36mModel.__deepcopy__\u001b[1;34m(self, memo)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__deepcopy__\u001b[39m(\u001b[38;5;28mself\u001b[39m, memo):\n\u001b[0;32m    380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt:\n\u001b[1;32m--> 381\u001b[0m         new \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeserialize_model_from_bytecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpickle_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserialize_model_as_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m         memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;66;03m# See comment in __reduce__ for explanation\u001b[39;00m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\pickle_utils.py:48\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     46\u001b[0m     model \u001b[38;5;241m=\u001b[39m saving_lib\u001b[38;5;241m.\u001b[39mload_model(filepath)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\pickle_utils.py:46\u001b[0m, in \u001b[0;36mdeserialize_model_from_bytecode\u001b[1;34m(serialized_model)\u001b[0m\n\u001b[0;32m     40\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(serialized_model)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# When loading, direct import will work for most custom objects\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;66;03m# though it will require get_config() to be implemented.\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Some custom objects (e.g. an activation in a Dense layer,\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;66;03m# serialized as a string by Dense.get_config()) will require\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# a custom_object_scope.\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\saving\\experimental\\saving_lib.py:202\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects)\u001b[0m\n\u001b[0;32m    200\u001b[0m _SAVING_V3_ENABLED\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m saving_v3_enabled_value\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(temp_path):\n\u001b[1;32m--> 202\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py:676\u001b[0m, in \u001b[0;36mdelete_recursively_v2\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.rmtree\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdelete_recursively_v2\u001b[39m(path):\n\u001b[0;32m    668\u001b[0m   \u001b[38;5;124;03m\"\"\"Deletes everything under path recursively.\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \n\u001b[0;32m    670\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 676\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeleteRecursively\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionDeniedError\u001b[0m: Failed to delete a file: C:\\Users\\capek\\AppData\\Local\\Temp\\tmp2agw6zos/variables.h5; Permission denied"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6c6261d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n\u001b[0;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mAdam(\u001b[38;5;241m3e-5\u001b[39m))\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_text\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\pycharm_projects\\Transformers_for_czech_language\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(np.array(df.text.apply(tokenize_text)), np.array(df.target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "47756271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kočka</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>velbloud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>atom</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>počítač</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>kalkulačka</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          text  target\n",
       "0        Kočka       1\n",
       "1          pes       1\n",
       "5     velbloud       1\n",
       "15        atom       0\n",
       "11     počítač       0\n",
       "14  kalkulačka       0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa533509",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
